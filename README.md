# DataEngineeringFundamentals
Files and notes about data engineering fundamentals

# Data engineering
## Simple definition: 
The process used to collect and validate quality data, which can be used by data analysts and data scientists. 
A bunch of techniques and procedures to turn available data into useful data.

# Data Pipeline
It's a way to move data from a local (origin) to a destination (a data warehouse or data lake, for example). In this process, the data are transformed and optimized, arriving in a state in which they can be analyzed and used for the development of business insights.
Pipeline it's a concept.

# Basic components of a data pipeline
- Origin
- Processing
- Destiny

# Pipeline ETL (extract, transform, load):
It's a type of data pipeline, but it is only a subprocess of a data pipeline. ETL is a term created in an era when the only destination was a data warehouse and the process was less complex. Today, ETL is a part of a greater data pipeline process.
A data pipeline can be created for batch data, streaming data (real time), or both.

